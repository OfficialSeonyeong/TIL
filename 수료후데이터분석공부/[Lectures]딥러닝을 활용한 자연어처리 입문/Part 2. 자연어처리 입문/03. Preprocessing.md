# 03. Preprocessing

## 01. 전처리 파이프라인

- 전처리는 반복적이고 끝이 없는 작업이지만 가장 중요하다.
- 문제 정의 → 데이터 수집 → 데이터 전처리 및 분석 → 알고리즘 적용 → 평가 → 배포
- 데이터(코퍼스)수집 → 정제 → 라벨링 → Tokenization → (Subword Segmentation) → Batchify



### Corpus(말뭉치)

자연어처리를 위한 문장들로 구성된 데이터셋

- Parallel Corpus: 대응되는 문장 쌍이 labeling 되어 있는 형태
- 포함된 언어 숫자에 따라, Monolingual Corpus, Bi-lingual Corpus, Multilingual Corpus



## 02. 코퍼스 수집

- 구입 : 양질의 데이터, 양이 제한적
- 외주 : 높은 비용, 인력이 추가 필요
- 무료 공개 데이터 : AI-HUB, WMT competition, Kaggle, OPUS, 양이 제한적
- 웹 크롤링 : 무한한 양의 코퍼스 수집 가능, 품질이 천차만별, 정제에 많은 노력 필요
  - 블로그, 뉴스 기사, Wikipedia, 커뮤니티, 자막...



## 03. 코퍼스 정제

- Task에 따른 노이즈 제거
  1. 기계적인 노이즈 제거
     1. 전각 문자 제거
     2. 대소문자 통일
     3. 정규식을 활용
  2. Interactive 노이즈 제거
     1. 정규식을 활용
     2. 노이즈 전부를 제거하는 것은 어려움

풀고자 하는 문제의 특성에 따라 전처리 전략이 다르다. → 이모티콘이 필요 없는 정보가 아닐 때도 많다.

- 인코딩 변환



## 04. 정규식 (Regular Expression)

1. Text Editor 활용

- 파일을 열어 적용 과정을 보면서 정제
- 바로 결과 확인
- 적용 과정이 log로 남지 않음

1. 전용 모듈 작성 및 활용

- Python등을 활용해 모듈을 만들고 regex 리스트를 파일로 받아서 처리
- 한번에 모든 regex를 적용
- 재활용 가능



### 정규식

- [ ]

   : 원소 중의 character = one of

  - `[2345cde]`은 2,3,4,5,c,d,e 중의 character

- [-]

   : 해당 연속 원소 중의 character = one of

  - `[2-5c-e]`은 2,3,4,5와 c,d,e 중의 character

- [^]

  : 원소 중에 없는 character = None of

  - `[^2-5c-e]`은 2,3,4,5와 c,d,e를 제외한 모든 character

- ( )

   : 각각에 지정

  - `(x)(yz)`은 x를 \1에 지정, yz를 \2에 지정

- |  : 둘 중에 하나, 그리고 \1에 지정

  - `(x|y)`은 x 또는 y

- **?** : 0번 또는 1번 나타남  ex. X?

- **+** : 1번 이상 나타남  ex. X+

- ***** : 나타나지 않을 수도, 반복될 수도 있음  ex. X*

- **{n}** : n번 반복  ex. X{n}, 카드 번호 [0-9]{4}

- **{n,}** : n번 이상 반복  ex. X{n,}

- **{n, m}** : n번부터 m번까지 반복  ex. X{n,m}

- **.** : any character

- **^ $** : 문장의 시작과 끝  ex. ^x$

- \s : 공백 문자

- \S: 공백 문자를 제외한 모든 문자

- \w: alphanumeric(알파벳+숫자)+'*' ([A-Za-z0-9*]와 같음)

- \W: non-alphanumeric 문자 및 '*' 제외 ([^A-Za-z0-9*]와 같음)

- \d: 숫자 ([0-9]와 같음)

- \D : 숫자를 제외한 모든 문자 ([^0-9]와 같음)



## 05. 실습 정규식 실습



## 06. 코퍼스 레이블링

- Text Classification (Sentence → Class)
- Token Classification (Sentence → Sequence)
- Sequence-to-sequence (Sequence → Sequence)



## 07. 한, 중, 영, 일 코퍼스 분절 (tokenization)

두 개 이상의 다른 token들의 결합으로 이루어진 단어를 쪼개어, vocabulary 숫자를 줄이고, 희소성(sparseness)을 낮추기 위해 tokenization을 해야 한다.

### 한국어 분절

- 어근에 접사가 붙어 다양한 단어가 파생된다. (교착어)
- 띄어쓰기 통일의 필요성

### 영어 분절

- NLTK를 사용해 comma 등 후처리

### 중국어/일본어 분절

- 기본적인 띄어쓰기 없음

### 형태소 분석 및 품사 태깅

형태소를 비롯해, 어근, 접두사와 접미사, 품사 등 다양한 언어적 속성의 구조를 파악하는 것이 형태소 분석이다. 형태소의 뜻과 문맥을 고려해 그것에 마크업을 하는 것을 품사 태깅이라고 한다.

### POS Tagger

한국어 - Mecab, KoNLPy

일본어 - KoNLP

중국어 - standard Parser, PKU Parser, Jieba



## 08. 실습 형태소 분석기를 활용한 분절하기



## 09. 분절 길이에 따른 장단점

토큰 평균 길이에 따른 성격과 특징

짧을수록

- Vocabulary 크기 감소 → 희소성 문제 감소
- OoV가 줄어든다.
- Sequence의 길이가 길어짐 → 모델의 부담 증가

길수록

- Vocabulary 크기 증가 → 희소성 문제 증대
- OoV가 늘어난다.
- Sequence의 길이가 짧아짐 → 모델의 부담 감소

*OoV: Out of Vocabulary로, 훈련 데이터 상에는 없지만 테스트 데이터에 존재하는 토큰*

*—특히 NLG, 이전 단어들을기반으로 다음 단어를 예측하는 task에서는 치명적*

정보량에 따른 이상적인 형태

- 빈도가 높을 수록 하나의 token으로 나타내고, 빈도가 낮으면 빈도가 높은 tokens로 구성한다.
- 압축 알고리즘과 유사



## 10. 서브워드 분절

subword 는 단어보다 더 작은 의미 단위로 모여 구성

### **Byte Pair Encoding(BPE) 알고리즘**(압축 알고리즘) 활용한 subword segmentation

- 학습 코퍼스를 활용해 BPE 모델 학습 후, 학습/테스트 코퍼스에 적용
- 장점
  - 희소성을 통계를 기반하여 효과적으로 낮출 수 있다.
  - 언어별 특성에 대한 정보 없이, 더 작은 의미 단위로 분절할 수 있다.
  - OoV를 없앨 수 있다.
- 단점
  - 학습 데이터 별로 BPE 모델로 생성됨
- Training
  1. 단어 사전 생성 (빈도 포함)
  2. Character 단위로 분절 후, pair별 빈도 카운트
  3. 최빈도 pair를 골라 merge 수행
  4. 3번 반복
- Applying
  1. 각 단어를 chracter 단위로 분절
  2. 단어 내에서 '학습 과정에서 merge에 활용된 pair의 순서대로 merge 수행

Subword Segmentation Modules

- Subword-nmt : https://github.com/rsennrich/subword-nmt
- SentencePiece: https://github.com/google/sentencepiece

—한국어의 경우, 띄어쓰기가 제멋대로인 경우가 많아서 바로 subword segmentation을 적용하는 것은 위험하다. 형태소 분석기를 통한 tokenization을 진행 후 적용하는 것을 권장



## 11. 실습 Subword segmentation



## 12. 분절 복원 (detokenization)

### Tokenization

1. 영어 원문 → 2. tokenization을 수행하고, 기존 띄어쓰기와 구분을 위해 `_` 삽입 → 3. subword segmentation을 수행, 공백 구분 위한 `_` 삽입

### Detokenization

1. whitespace 제거 → 2. `__`을 white space로 치환 → 3. `_`를 제거



## 13. 실습 분절 복원

```python
import sys

STR = '_'
TWO_STR = '__'

def detokenization(line):
	if TWO_STR in line:
		line = line.strip().replace(' ', '').replace(STR, '').strip()
	else:
		line = line.strip().replace(' ','').replace(STR, ' ').strip()
	return line

if __name__ == "__main__:":
	for line in sys.stdin:
		if line.strip() != "":
				buf = []
				for token in line.strip().split('\\t'):
					buf += [detokenization(token)]
				sys.stdout.write('\\t'.join(buf) + '\\n')
			else:
				sys.stdout.write('\\n')
```



## 14. 병렬 코퍼스 정렬 시키기

⇒ Paralled Corpus

### Champollion

- 단어 번역 사전에 기반하여 사전을 최대한 만족하는 sentence align을 찾는 방식
- Perl로 제작됨
- ratio parameter의 역할: source 언어의 character 당 target 언어의 character 비율

단어 사전 → MUSE 활용

### [Procedure]

1. Bi-lingual Corpus 정제(노이즈 제거)
2. Tokenization 수행
3. 각 언어별 코퍼스에 대해서 word embedding 수행(FastText 활용)
4. MUSE를 활용하여 word translation dictionary 추출
5. Champollion을 활용해 align 수행



## 15. TIP 전처리의 중요성, 경험담

- 오픈 소스 문화의 확산(⇒ Continuous renewal of state of the art)
- 데이터가 더 큰 자산



## 16. 미니배치 만들기

1. 빈도 순으로 단어 사전 정렬

2. min_count보다 작은 빈도를 갖는 단어 제외 or max_vocab 에따라 빈도순으로 어휘 제외

3. 필요에 따라 특수 토큰도 어휘 사전에 포함 ex. <BOS>, <EOS>, <UNK>, <PAD>

4. 미니배치의 형태를 `(batch_size, length, |V|)` → `(batch_size, length,1)`

   one-hot 벡터를 다 저장할 필요가 없기 때문

5. Sequence 차원의 크기는 미니배치 내의 가장 긴 문장에 의해 결정된다. → <PAD> 토큰이 필요

   - Increase Training Efficiency → 비슷한 길이의 sequences 별로 chunks 만들기
   - 그래서 미니배치별로 shuffling해서 학습



### TorchText

PyTorch 공식 텍스트 로딩용 라이브러리



### Step 1 :  Define Fields

```python
self.label = data.Field(sequential=False, use_vocab=True, unk_token=None)
self.text = data.Field(use_vocab=True, batch_first=True, 
											 include_lengths=False, eos_token='<EOS>' if use_eos else None)
```



### Step 2: Define Dataset with Fields

```python
train, valid = data.TabularDataset.splits(path='', train=train_fn, validation=valid_fn, 
																					format='tsv', fields==[('label', self.label), ('text', self.text)])
```



### Step 3: Get DataLoaders from Datasets

```python
self.train_iter, self.valid_iter = data.BucketIterator.splits((train, valid), batch_size=batch_size, 
																																device='cuda:%d' % device if device >=0 else 'cpu',
																																shuffle=shuffle, sort_key=lambda x: len(x.text), sort_within_batch=True)
self.label.build_vocab(train)
self.text.build_vocab(train, max_size=max_vocab, min_freq=min_freq)						
```



After

```python
vocab_size = len(dataset.text.vocab)
n_classes = len(dataset.label.vocab)
print('|vocab| =', vocab_size, '|classes| =', n_classes)

trainer.run(train_loader,max_epochs=self.config.n_epochs)
```



## 17. 실습 TorchText

```
wc -l ./review.sorted.uniq.refined.tok.*
# 데이터 크기

shuf <review.sorted.uniq.refined.tok.tsv> review.sorted.uniq.refined.tok.shuf.tsv
# shuffling

head -n 62943 ./review.sorted.uniq.refined.tok.shuf.tsv > review.sorted.uniq.refined.tok.shuf.train.tsv
# train set 생성

head -n 10000 ./review.sorted.uniq.refined.tok.shuf.tsv > review.sorted.uniq.refined.tok.shuf.test.tsv
# test set 생성
```

이후 실습 파일에서 진행