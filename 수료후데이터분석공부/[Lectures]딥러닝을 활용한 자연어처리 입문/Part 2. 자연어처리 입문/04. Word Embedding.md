# 04. Word Embedding



## 01. 들어가며

단어는 discrete하고 categorical 하지만, 유사성을 지닌 단어가 있음.

**Feature(특징)**

- 샘플을 잘 설명하는 특징
- 특정 샘플을 수치화할 수 있다.

**Dimension Reduction**

x의 feature를 추출하는 방법을 학습

**Word Embedding**

- 단어를 연속적인 값으로 표현
- dense vector를 얻을 수 있고, 단어의 필요한 특징을 잘 표현



## 02. Word Sense

- Word → Lemma(여러 개) → Sense(여러 개)
- 다의어(Polysemy)와 동형어(Homonym) → Word Sense Disambiguation 필요
- 동의어(Synonyms)와 반의어(Antonyms)
- 상위어(Hypernyms)과 하위어(Hyponyms): 계층적 구조, 상위 개념

⇒ 이러한 단어를 표현하기 위한 방법 필요 (one-hot 인코딩은 불가)



## 03. WordNet

- Thesaurus(어휘분류사전, 시소러스) 기반 방식
- 동의어 집합(Synset) 또는 상위어(Hypernym)나 하위어(Hyponym)에 대한 정보가 특히 잘 구축되어 있다.
- 단어 사이의 거리를 구할 수 있다.
- $similarity(w, w\prime) = -\log distance(w, w\prime)$
- 기존 사전을 활용하는 원리이기 때문에 데이터 입력 시 발생하는 편차 또는 오류 발생, 신조어나 사전에 등록되지 않은 단어는 적용 불가 등의 한계가 있다.



## 04. 실습 WordNet을 활용한 단어 유사도 계산

jupyter notebook으로 진행



## 05. 딥러닝 이전의 단어 임베딩

### Traditional Methods to make word vectors

- 데이터 기반 방식

1. TF-IDF
   - 텍스트 마이닝(Text Mining)에서 중요하게 사용
   - 어떤 단어 w가 문서 d 내에서 얼마나 중요한지 나타내는 수치
   - TF(Term Frequency)
     - 단어의 문서 내에 출현한 횟수
     - 숫자가 클수록 문서 내에서 중요한 단어
     - 하지만, 'the'와 같은 단어도 매우 큰 값을 가질 것이다.
   - IDF(Inverse Document Frequency)
     - 그 단어가 출현한 문서의 숫자의 역수(Inverse)
     - 값이 클수록 일반적으로 많이 쓰이는 단어
   - $TF-IDF(w,d)= \frac{TF(w,d)} {DF(w)}$
   - TF-IDF Matrix: 단어의 각 문서(문장, 주제)별 TF-IDF 수치를 vector화
2. Based on Context Window(Co-occurence)
   - 함께 나타나는 단어들을 활용
   - Context Window를 사용해 windowing 실행 - 적절한 window 크기가 중요

- 데이터 기반으로 코퍼스(or 도메인) 특화된 표현 가능
- 여전히 sparse한 vector로 표현된다. - 차원 축소를 통해 해결 가능



## 06. 단어간 유사도(거리) 구하기

1. Manhattan Distance(L1 distance)

$$d_{L1}(w,v) = \sum^d_{i=1} |w_i - v_i|, \space where \space w,v \in R^d.$$

1. Euclidean Distance(L2 distance)

$$d_{L2}(w,v)=\sqrt{\sum^d_{i=1} (w_i-v_i)^2}, where\space w,v \in R^d.$$

1. Infinity Norm

$$d_\infin(w,v)=max(|w_1-v_1|, |w_2-v_2|, ... ,|w_d-v_d|), where \space w, v \in R^d.$$

1. Cosine Similarity (벡터의 방향까지 고려)



## 07. 실습 딥러닝 이전의 단어 임베딩 구현하기

jupyter notebook으로 진행



## 08. Word2Vec

- 가정: 주변에 같은 단어가 나타나는 단어일수록 비슷한 벡터 값을 가져야 한다.
- 문장의 문맥에 따라 정해지는 것이 아니다.
- 방법: CBOW와 Skip-gram
- Skip-gram
  - 주변 단어를 예측하도록 하는 과정에서 적절한 단어의 임베딩(정보의 압축)을 할 수 있다.
  - 오토인코더와 기본적인 개념이 비슷하다.
  - Non-linear function이 없는 classification task
  - 쉽고 빠르며, 비교적 정확한 벡터를 구할 수 있다.
  - 느리고, 출현 빈도가 적은 단어일 경우 벡터가 정확하지 않다.



## 09. GloVe

- 단어 x와 윈도우 내에 함께 출현한 단어들의 출현 빈도를 맞추도록 훈련 → Regression task
- 출현 빈도가 적은 단어에 대해서는 loss의 기여도를 낮춘다.
- 전체 코퍼스에 대해 각 단어 별 co-occurrence를 구한 후, regression을 수행