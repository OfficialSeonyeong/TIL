# 02. Introduction

## 01. 자연어처리란 무엇인가

- 언어란 정보를 압축해서 전달하기 위한 도구
- 자연어란 사람들이 일상적으로 쓰는 언어를 인공적으로 만들어진 언어인 인공어와 구분하여 부르는 개념 (위키피디아)
- 자연어처리란 사람이 이해하는 자연어를 컴퓨터가 이해할 수 있는 값으로 바꾸는 과정(NLU)
- 더 나아가 컴퓨터가 이해할 수 있는 값을 사람이 이해하도록 바꾸는 과정(NLG)



## 02. NLP with Deep Learning

### Traditional NLP

- 단어를 symbolic 데이터로 취급 → 단어의 연관성을 무시
- 여러 sub-module을 통해 전체 구성
- 디버그 용이



### NLP with Deep Learning

- 단어를 continuous value로 변환 (ex. Word2Vec)
- End-to-end 시스템 추구
- 디버깅 어려움



### Paradigm Shift in NLP

- 효율적인 Embedding을 통한 성능 개선
- End-to-end 구성으로 인한 효율/성능 개선



## 03. 자연어처리와 다른 분야의 차이점

- Discrete value를 다룬다.
- 분류 문제로 접근할 수 있다.
- 샘플의 확률 값을 구할 수 있다.
- GAN 적용 불가
- Domain Knowledge인 언어적 지식 필요
- 전처리 과정이 어렵다.



## 04. 왜 자연어처리는 어려운가

1. Ambiguity - 중의성, 문장 내 정보 부족
2. Paraphrase -  같은 의미를 여러 다양한 문장으로 표현 가능
3. Discrete, not Continuous



## 05. 왜 한국어 자연어처리는 더 어려운가

**교착어 -** 어간에 접사가 붙어 단어를 이루고 의미와 문법적 기능이 정해짐 → 한국어

**굴절어 -** 단어의 형태가 변함으로써 문법적 기능이 정해짐 → 라틴어, 독일어, 러시아어

**고립어** - 어순에 따라 단어의 문법적 기능이 정해짐 → 영어, 중국어



### 한국어의 특징

- 접사 추가에 따른 의미 파생
- 유연한 단어 순서 규칙
- 모호한 띄어쓰기
- 평서문과 의문문의 차이 부재
- 주어 부재
- 한자 기반의 언어(표의 문자인 한자 → 표음 문자인 한글)



## 06. 딥러닝 자연어처리 주제 및 역사

### Before Deep Learning

- 여러 단계의 sub-module로 구성되어 복잡한 디자인 구성

### Before Sequence-to-Sequence

- Text to Numeric values : Word Embedding, Text Classification

### After Sequence-to-Sequence with Attention

- Beyound Text to Numeric

### Era of Attention

- Transformer by End-to-End Attention

### Pretraining and Fine-tuning

- Transformer 기반 BERT를 활용한 Big Language Models



## 07. 최근 흐름

- Big Models를 활용해 transfer learning
- BERT의 개선을 위한 여러 가지 연구 진행
- 모델의 경량화
- (Open Domain) Dialogue System - chatbot
- Question Answering
- Knowledge Graphs
- NLG: Machine Transitions → unsupervised approach, machine translations for low-resources