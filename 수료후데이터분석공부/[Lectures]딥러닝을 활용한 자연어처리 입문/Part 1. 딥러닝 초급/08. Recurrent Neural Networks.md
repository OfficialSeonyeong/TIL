# 08. Recurrent Neural Networks

## 01. RNN 소개

- hidden state가 다시 입력 값으로 동작
- 순서 정보나 이전 정보에 기반한 현재 정보 처리 가능



### Sequential Data vs Time Series

Time-stamp의 유무에 따른 차이

- sequential data는 데이터의 순서 정보가 중요
- 텍스트, 영상, 음성
- 시계열 데이터는 해당 데이터가 발생한 시각 정보가 중요
- 주식 데이터, 센서 데이터



## 02. RNN Step-by-Step 들여다보기

$$
\hat y_t = h_t = f(x_t, h_{t-1}; \theta) \\ = tanh(W_{ih} x_t +b_{ih} + W_{hh}h_{t-1} + b_{hh} )
$$



### Single-layered RNN

Input Tensor :
$$
|x_t| = (batch \space size, n, input \space size) \\ |X| = (batch \space size, n, input \space size)
$$
Hidden Tensor:
$$
|h_t| = (batch \space size, hidden \space size) \\ |h_{1:n}| = (batch \space size, n, hidden \space size)
$$


### **Multi-layered RNN**

- 마지막 레이어의 hidden state가 y_hat
- 시간에 대해서는 파라미터가 같지만 층끼리는 다르다.
- 마지막 time-step의 hidden state가 모든 layer의 hidden state

Output Tensor:

- 1층의 출력이 2층의 입력 ... So, 마지막 출력이 y_hat

$$
|h_{1:n}| = (batch \space size, n, hidden \space size)
$$

Hidden State Tensor:
$$
|h_t| = (Number \space of \space layers, batch \space size, hidden \space size)
$$


### Bidirectional Multi-layered RNN

- 역방향과 정방향으로 작동한다.
- Output 사이즈는 hidden state의 2배가 된다.
- Hidden state는 layer 개수가 2배가 된다.

Output Tensor:
$$
|h_{1:n} | = (batch \space size, n, hidden \space size \times Number \space of \space directions)
$$
Hidden State Tensor:
$$
|h_t| = (Number  \space of \space directions \times Number \space of \space layers, batch \space size, hidden \space size)
$$


## 03. RNN 활용 사례

### 1. Many to One

- ex) Text Classification

### 2. One to Many

- ex) NLG, Machine Translation

### 3. Many to Many

- ex) POS Tagging, MRC



### Two Approaches

1. Non-autoregressive(Non-generative)

- 현재 상태가 앞과 뒤 상태를 통해 정해지는 경우
- POS Tagging, Text Classification
- Bidirectional RNN 사용 권장

1. Autoregressive(Generative)

- 현재 상태가 과거 상태에 의존하여 정해지는 경우
- NLG, Machine Translation
- One-to-Many case
- Bidirectional RNN 사용 불가



## 04. RNN에서의 Back-propagation (BPTT)

**FC layers**에서는 hidden state가 더해져 마지막 y_hat을 구한다. y 값과의 loss 값을 구한 뒤 경로 별로 back propagation 되어 gradient가 더해진다.

**Many to One**에서는 동일한 parameter의 gradient를 모두 더해서 구한다. sequence의 길이가 길면 또는 time stamp의 길이가 길면 gradient의 크기가 커진다.

**Many to Many**에는 모든 time-step에 대해 y_hat 값이 나온다. 모든 값의 loss를 더해 back propagation 수행, 하나의 time-step에서 도출되는 두 가지 값 모두 더해진다.

**Many to Many with Multi-layered RNN**에는 한 셀마다 경로가 많다. 이 모든 값이 다 더해진다.

RNN은 TanH를 사용하는데 깊은 네트워크를 구축할 때 여러 번 사용하게 되어 Gradient Vanishing 문제를 일으키게 된다. ThnH는 0일 때를 제외하고 Gradient가 1보다 작다. ⇒ 긴 시퀀스의 학습이 어려워진다.



## 05. 수식 BPTT

<img src="..\Images\RNN_수식 BPTT_Many to One.PNG" style="zoom:60%;" />



<img src="..\Images\RNN_수식 BPTT_Many to Many.PNG" style="zoom:60%;" />



### Gradient Vanishing

$$
{\alpha L(\theta) \over \alpha \psi} = \sum_{t=1} ^T {\alpha L(\theta) \over \alpha \hat y_4 } {\alpha \hat y_4 \over \alpha h_4} (\Pi_{i=t}^{T-1} {\alpha h_{i+1} \over \alpha h_i}) {\alpha h_t \over \alpha \psi} \\ {\alpha h_{t+1} \over \alpha h_t} \le 1
$$



## 06. Long-Short Term Memory (LSTM)

RNN 내부에는 tanh가 있어 time-step이 길어짐에 따라 gradient vanishing이 발생한다.

⇒ **Sigmoid를 활용한 Gate**

Sigmoid는 0과 1사이의 값을 반환하므로, sigmoid를 곱하면 마치 문을 열고 닫는 듯한 효과를 낼 수 있다.
$$
y = \sigma (x) \times f(x)
$$

- LSTM은 weight parameter가 많고 무거워서 굉장히 복잡하다.



### Gated Recurrent Unit(GRU)

- LSTM보다 훨씬 가볍다.
- 그러나 실제로 LSTM이 더 널리 쓰이는 추세
- LSTM은 vanilla RNN에 비해 훨씬 많은 파라미터를 가진다. ⇒ 더 많은 학습 데이터와 학습 시간이 필요
- Gradient Vanishing 문제를 해결했지만, 무조건 긴 데이터를 모두 기억할 수 있는 것은 아니다. ⇒ Attention을 통해 이를 해결할 수 있다.



## 07. Gradient Vanishing과 LSTM

- 포겟 게이트 레이어가 열고 닫으면서 배우는 거에 따라 미분 → gradient vanishing 어느 정도 해결
- 하지만, layer를 깊게 구축할 때도 Gradient Vanishing 문제가 생긴다.



## 08. 실습 브리핑

Bidirectional RNN 사용 ← 한번에 데이터를 모두 학습하기 때문

마지막 time-step의 출력 (batch_size, 1, hidden_size * 2)



## 09. 실습 LSTM으로 MNIST 분류 구현하기

visual studio code로 진행



## 10. Gradient Clipping

- RNN은 BPTT 알고리즘에 따라, time-step이 많아질수록 gradient의 덧셈이 많아짐 → 시퀀스가 길어지면 gradient가 커진다.

- 시퀀스의 길이에 따라 적절한 learning rate를 바꾸는 방법 적용 ⇒ **Gradient Clipping**

- $$
  {||\triangledown_\theta L(\theta)||_2 \over \tau} = k
  $$

- Threshold와의 크기 비교, k의 값 활용

- k가 1보다 크면, k로 나눠주면 방향은 유지하고 기울기 크기만 줄어든다. → 원래 크기로서는 1 step이 커서 minimum을 놓칠 수 있는데 안정적인 step 크기로 만들어준다.

- learning rate보다 threshold의 크기를 튜닝한다.

- 시퀀스의 길이에 따라 gradient의 크기가 달라져 gradient exploding 예방한다.

- Adam optimizer를 사용할 경우 큰 필요가 없다.