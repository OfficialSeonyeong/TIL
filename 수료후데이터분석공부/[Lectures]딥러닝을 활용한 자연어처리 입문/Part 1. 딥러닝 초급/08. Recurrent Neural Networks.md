# 08. Recurrent Neural Networks

## 01. RNN 소개

- hidden state가 다시 입력 값으로 동작
- 순서 정보나 이전 정보에 기반한 현재 정보 처리 가능



### Sequential Data vs Time Series

Time-stamp의 유무에 따른 차이

- sequential data는 데이터의 순서 정보가 중요
- 텍스트, 영상, 음성
- 시계열 데이터는 해당 데이터가 발생한 시각 정보가 중요
- 주식 데이터, 센서 데이터



## 02. RNN Step-by-Step 들여다보기

$$
\hat y_t = h_t = f(x_t, h_{t-1}; \theta) \\ = tanh(W_{ih} x_t +b_{ih} + W_{hh}h_{t-1} + b_{hh} )
$$



### Single-layered RNN

Input Tensor :
$$
|x_t| = (batch \space size, n, input \space size) \\ |X| = (batch \space size, n, input \space size)
$$
Hidden Tensor:
$$
|h_t| = (batch \space size, hidden \space size) \\ |h_{1:n}| = (batch \space size, n, hidden \space size)
$$


### **Multi-layered RNN**

- 마지막 레이어의 hidden state가 y_hat
- 시간에 대해서는 파라미터가 같지만 층끼리는 다르다.
- 마지막 time-step의 hidden state가 모든 layer의 hidden state

Output Tensor:

- 1층의 출력이 2층의 입력 ... So, 마지막 출력이 y_hat

$$
|h_{1:n}| = (batch \space size, n, hidden \space size)
$$

Hidden State Tensor:
$$
|h_t| = (Number \space of \space layers, batch \space size, hidden \space size)
$$


### Bidirectional Multi-layered RNN

- 역방향과 정방향으로 작동한다.
- Output 사이즈는 hidden state의 2배가 된다.
- Hidden state는 layer 개수가 2배가 된다.

Output Tensor:
$$
|h_{1:n} | = (batch \space size, n, hidden \space size \times Number \space of \space directions)
$$
Hidden State Tensor:
$$
|h_t| = (Number  \space of \space directions \times Number \space of \space layers, batch \space size, hidden \space size)
$$


## 03. RNN 활용 사례

### 1. Many to One

- ex) Text Classification

### 2. One to Many

- ex) NLG, Machine Translation

### 3. Many to Many

- ex) POS Tagging, MRC



### Two Approaches

1. Non-autoregressive(Non-generative)

- 현재 상태가 앞과 뒤 상태를 통해 정해지는 경우
- POS Tagging, Text Classification
- Bidirectional RNN 사용 권장

1. Autoregressive(Generative)

- 현재 상태가 과거 상태에 의존하여 정해지는 경우
- NLG, Machine Translation
- One-to-Many case
- Bidirectional RNN 사용 불가