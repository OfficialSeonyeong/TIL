# 02. Representation Learning



## 01. 특징(feature)이란

- 샘플을 잘 설명하는 특징
- 특징을 통해 특정 샘플을 수치화할 수 있다.
- 딥러닝에서는 hand-crafted feature가 필요하지 않다.
  - Raw 데이터에 최소한의 전처리를 수행
  - 미처 발견하지 못한 특징도 활용 가능하지만 사람이 해석하기 어려움
- feature vector를 통해 샘플 사이의 거리(유사도) 계산 가능



## 02. 원핫 인코딩

- Categorical(Discrete) Value를 나타내기 위한 방법
- Categorical Value는 Continuous Value와는 다르게 비슷한 값일지라도 상관없는 의미를 지닌다.
- **1개의 1과 n-1개의 0으로 이루어진 n차원의 벡터**
- Sparse Vector이다. → 벡터의 대부분 element가 0으로 이루어진 희소 벡터
- [*문제점 존재*] 서로 다른 두 벡터는 항상 직교(orthogonal)한다. ⇒ Cosine similarity가 0 ⇒ 두 샘플 사이의 유사도(거리)를 구할 수 없다.
- NLP에서는 Word2Vec를 통해 차원 축소 및 **dense vector(vector embedding)**로 표현



## 03. 오토 인코더

- **input과 output이 동일하도록 하는 과정!**

- 인코더(encoder)와 디코더(decoder)를 통해 압축과 해체 실행

  <img src="../Images/autoencoder.png" style="zoom:60%;" />

  - 인코더에서 입력의 정보를 최대한 보존하도록 손실 압축 수행
  - 중간 결과물(z)는 정보의 선택과 압축 발생, 그리고 입력에 대한 feature vector라고 할 수 있다. (dense vector일 것이다.)
  - 디코더는 중간 결과물(z)의 정보를 입력과 같아지도록 압축 해제(복원) 수행

- 복원을 성공적으로 하기 위해 오토 인코더(auto encoder)는 특징(feature)을 추출하는 방법을 자동으로 학습



## 04. Hidden Representations

- 인코더의 결과물 z를 plot 하면 비슷한 샘플들은 비슷한 곳에 위치하게 된다. → 이 공간을 hidden(latent) space라고 한다.
- 각 레이어의 결과물을 hidden vector라고 부른다.
- 해석이 불가능하다. 하지만 비슷한 특징을 가진 샘플은 비슷한 hidden vector를 가진다.