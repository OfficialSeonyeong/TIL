# 06. Convolutional Neural Networks

## 01. CNN 소개

- 딥러닝 이전에는 이미지에서 edge를 Hand-crafted feature 추출 후 모델링
- Convolution filter가 입력에 대해서 돌아가며 동작
  - Weight Parameter가 kernel이 된다.
- 끝부분도 입력을 잘하기 위해서 모서리에 Pad를 더해 output 사이즈를 input 사이즈와 동일하게 만들 수 있다. (padding)
- 특정 부분에만 값이 있는 kernel을 넣어 히트맵을 만들고, 여러 개의 히트맵을 가지고 모델은 라벨을 추측한다.
- \#input channels →  # output channels(#kernels) ⇒ #kernels (#kernels과 동일)
- Input & output Tensors의 사이즈는 (Batch_size, #Channels(Color), Height, Width)

⇒ 이미지에 굉장히 특화된 방법



### 입출력 크기 계산법

$$
b = batch \space size \\ (x_{height}, x_{width}) = input \space size \\ c_{in} = Number \space of \space input \space channels \\ c_{out} = Number \space of \space output \space channels \\ (k_{height}, k_{width}) = kernel \space size \\ (y_{height}, y_{width}) = output \space size \\ \\ y = conv2d(x), \\ where \space |x| = (b, c_{in}, x_{height}, x_{width}) \\ |y| = (y_{height}, y_{width}) \\ = (b, c_{out}, x_{height} - k_{height} +1, x_{width} - k_{width} +1).
$$



### 입출력 크기 계산 방법(+pad)

$$
b = batch \space size \\ (x_{height}, x_{width}) = input \space size \\ c_{in} = Number \space of \space input \space channels \\ c_{out} = Number \space of \space output \space channels \\ (k_{height}, k_{width}) = kernel \space size \\ (y_{height}, y_{width}) = output \space size \\ (p_{height}, p_{width}) = pad \space size \\ y = conv2d(x), \\ where \space |x| = (b, c_{in}, x_{height}, x_{width}) \\ |y| = (y_{height}, y_{width}) \\ = (b, c_{out}, x_{height} +2\times p_{height} - k_{height} +1, x_{width}  +2\times p_{width}- k_{width} +1).
$$



### Convolution Layer의 특징

- Feature의 위치에 구애 받지 않는다.
- 같은 입출력을 갖는 FC Layer에 비해 더 적은 weight를 가진다.
- 병렬 계산 구성이 쉬우므로, GPU에서의 연산이 매우 빠르다.
- 하지만 FC Layer에 비해 입출력 크기가 계산이 까다로워, 네트워크 구성이 쉽지 않다.



## 02. CNN 활용 사례

음성 인식 → 시간과 빈도 수로 시각화해서 CNN을 통해 추측

NLP → Text Classification:문장을 kernel이 훑으면서 계산, Machine Translation

시계열 → (bs, length, 1)의 Tensor로 패턴 학습



## 03. Max-pooling & Stride

pad를 더하게 되면 입출력 텐서의 크기가 동일해짐 ( 3X3 + 1 padding)

⇒ **Dimension Reduction**을 위해 Max-pooling & Stride 수행



### Max-pooling

- Down sampling 기법

  출력 텐서에서 특정 크기의 범위에서 해당 값 중 최댓값만 출력

- 별도의 Max-pooling Layer를 더해야 함



### Stride

- 요즘 조금 더 선호되는 방법

- 같은 conv layer 내에서 속성 값을 지정해 동작

- CNN 연산 과정에서 kernel이 몇 칸을 건너뛰며 계산해 출력 텐서의 크기가 작아짐

- 입출력 크기 계산법
  $$
  b = batch \space size \\ (x_{height}, x_{width}) = input \space size \\ c_{in} = Number \space of \space input \space channels \\ c_{out} = Number \space of \space output \space channels \\ (k_{height}, k_{width}) = kernel \space size \\ (y_{height}, y_{width}) = output \space size \\ (p_{height}, p_{width}) = pad \space size \\ (s_{height}, s_{width}) = stride \space size \\ y = conv2d(x), \\ where \space |x| = (b, c_{in}, x_{height}, x_{width}) \\ |y| = (y_{height}, y_{width}) \\ = (b, c_{out}, [(x_{height} +2\times p_{height} - (k_{height}-1)-1)/s_{height} +1], [(x_{width}  +2\times p_{width}- (k_{width}-1)-1)/s_{width} +1]).
  $$
  

## 04. 실제 구현할 때 팁

FC Layer에 비해 입출력 크기가 계산이 까다로움

*전형적인* **CNN Block**

1. 3 X 3 Convolution Layer (+pad)
2. ReLU
3. Batch Normalization
4. 3 X 3 Convolution Layer (+pad) (+with Stride size (2X2))
5. ReLU
6. Batch Normalization
7. (+Max-pooling if no stride)

⇒ Input (1, C, H, W) → CNN Block → (64, H/2, W/2) → CNN Block →(128, H/4, W/4) →  CNN Block → (256, 1, 1) → FC Layers → Softmax



## 05. 실습 브리핑

기존 FCNN에서 CNN으로 업그레이드시킨다.

(N, C, 28, 28) → (32, 14, 14) → (64, 7, 7) → (128, 4, 4) → (256, 2, 2) → (512, 1, 1) → FC Layer (1, 50) → softmax (1, 10)



## 06. 실습 CNN으로 MNIST 분류 구현하기

jupyter notebook, visual studio code로 진행