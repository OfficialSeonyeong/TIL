# 04. Geometric Perspective



## 01. 차원의 저주

- 차원이 높을수록 데이터는 희소하게 분포하게 되어 학습이 어려워진다.

  → 차원이 높을수록 빈 공간이 많아지고 모든 구역을 살펴볼 때 효율적이지 못함.

- 차원이 작아도 좋지 않은 점은 같은 구역 내의 점들은 서로 구별할 수 없기 때문이다.

- 희소성이 높을수록 modeling의 난이도가 높아짐(Gaussian Mixture를 fitting하고자 할 때, K-means 클러스터링 수행 시)

- 따라서 데이터의 특징(feature)을 살리면서 낮은 차원에서 표현해야 한다.



## 02. 차원 축소

### Linear Dimension Reduction: PCA

n차원의 공간에 샘플들의 분포가 주어져 있을 때, 분포를 잘 설명하기 위한 새로운 axis를 찾아내는 과정

- Linear axis 위에 projection시켰을 때의 데이터 사이의 거리의 합이 최대가 되고, axis와 실제 데이터 상의 거리의 합이 최소가 되도록

Nonlinear Dimension Reduction은 딥러닝을 통해 가능하다.



## 03. 매니폴드(Manifold) 가설

데이터의 분포는 모든 차원에 uniform하게 분포해있지 않다.

**고차원 공간의 샘플들이 저차원 다양체(manifold)의 형태로 분포해 있다는 가정**

이 가정 하에 딥러닝의 비선형 차원 축소가 진행된다.



## 04. 실습 매니폴드 가설 실습

jupyter notebook으로 진행



## 05. 정리하며

- DNN은 굉장히 유연한 함수이기 때문에 다양한 관점에서 해석 가능
- 대부분의 새롭게 제시되는 방법들은 위의 관점에서 설계되고 제안된 것