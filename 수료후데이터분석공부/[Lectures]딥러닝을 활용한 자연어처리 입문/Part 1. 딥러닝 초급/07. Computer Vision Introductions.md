# 07. Computer Vision Introductions

## 01. 영상 처리 소개

- 사람이 보기에도 구별 어려운 경우, 데이터의 편향으로 잘못된 결과를 내는 경우가 있다.
- 딥러닝의 가장 큰 영향을 받은 Computer Vision
  - Image Classification
  - Object Detection
  - Image Segmentation
  - Image Generation



## 02. VGG 소개

- 고정된 아키텍처 네트워크 Backbone , 이를 주로 그대로 또는 변형해서 사용
- VGG는 2014년에 발표된 아키텍처로, 편리하고 가벼운 사용성으로 인기
- *Before VGG,* 5 X 5, 7 X 7 conv.layer 사용
- 3 X 3 layer를 반복 사용하면 5 X 5, 7 X 7 layer 대체 가능
  - 3 X 3 2번 → 5 X 5
  - 3 X 3 3번 → 7 X 7
- 결과적으로 더 깊은 네트워크 달성 가능, 더 적은 weight로 더 큰 capacity 달성



### Methodology

1. 3 X 3 conv.layer + 1 padding
2. 활성 함수 (Batch Normalization 추가 가능)
3. 필요에 따라 1~2번 반복
4. 2 X 2 maxpooling (stride로 대체 가능)

- 가볍고 편리하지만, 마지막 FC layer에 파라미터가 많이 들어가서 이 부분 학습이 단점



## 03. ResNet 소개

- 2015년에 등장해 152 layers를 가진 네트워크
- 깊은 네트워크가 학습이 잘되지만, 최적화 이슈, Gradient Vanishing 등 여러 애로사항이 많음
- 최적의 깊이가 존재할 텐데, 깊어지면 나머지는 identity 함수로 되면 되지 않을까?



### Methodology

$$F(x) = H(x) - x \\ H(x) = F(x) + x$$

최적의 깊이가 되고 나면 남은 함수들은 identity 함수가 되게끔 한다.

F(x)를 0으로 만들어 H(x) = x로 만든다.

⇒ Residual Block을 쌓자

- 입력값과 출력값의 차이만 학습 (F(x)가 0이 되는 방향으로)
- 입력값과 출력값의 차원이 같아야 더하기 연산이 가능한데, 다를 경우에는 W를 x에 곱해 더한다.



### Evaluation

기존: 레이어가 깊어질수록 낮은 성능

Resnet: 레이어가 깊어질수록 높은 성능

- Gradient Vanishing 을 완벽하게 방지해주는 방법
- 현재 제안되는 대부분의 큰 네트워크들은 residual connection을 차용



## 04. 전이학습(transfer learning) 소개

- NLP에서 유행하고 있는 학습 방법 (BERT, GPT 활용)
- 각 con. layer는 위치에 따라 low-level 또는 high-level feature를 추출하도록 학습됨
- **데이터가 다르더라도 이미지를 활용한 task에서는 공통된 feature들이 존재할 것이다.**
- 그래서 이미 Big Dataset으로 pretrained된 모델의 weights를 가져와서 활용 → Fine tuning



### How to apply

1. Set seed weights and train as normal
2. Fix loaded weights (freeze) and train unloaded parts
3. Train with different learning rate on each part



## 05. 실습 브리핑

강아지와 고양이 구별: Binary Classification

torchvision.models에 사전 구현된 모델을 가져와 Transfer Learning

Random Init 학습, Pretrained weight를 가지고 학습, Freezed하고 마지막 레이어만 더해 학습하는 방법 중 Freezed 학습법이 젤 성능이 좋았다.



## 06. 실습 백본 네트워크를 활용한 전이학습

visual studio code로 진행