# 03. Probabilistic Perspective



## 01. 들어가며

- 이 세상은 확률에 기반한다. → **확률 분포**를 학습해야 한다.
- 가상의 확률 분포 함수 `P(y|x)`를 근사할 것.



## 02. 기본 확률 통계

- 어떤 변수 x가 `x`라는 값을 가질 확률  `P(x = x)`
- 확률 분포 (함수)
  - 입력: 확률 변수 x
  - 출력: x가 각 값에 해당될 때에 대한 확률 값



### Discrete Probability Distribution

- 확률 값의 총 합은 1
- Probability Mass Function (확률 질량 함수, PMF)



### Continuous Probability Distribution

- 면적의 합이 1
- 함수 값이 1보다 클 수 있다.
- Probability Density Function (확률 밀도 함수, PDF)
- 어떤 샘플이 주어졌을 때, 확률 값을 알 수 없다.



### Joint Probability

결합 분포 `P(x,y)`



### Conditional Probability

조건부 확률 분포 `P(y|x) = P(x,y)/P(x)`  ⇒ `P(x,y) = P(y|x) P(x)`



### Bayes Theorem

데이터 D가 주어졌을 때, 가설 h의 확률

```
P(h|D) = P(D|h)P(h) / P(D)
```



### Marginal Distribution

결합 분포에서 한 변수를 적분한 형태
$$
P(x) = \int P(x,z) dz \\ =\int P(x|z)P(z) dz \\ = \int P(z|x)P(x) dz = P(x) \int P(z|x) dz
$$


### Expectation and Sampling

$$
E_{x \in P(x)} [f(x)] = \sum_{x \in X} P(x) \times f(x)\\P(x) = \int P(x,z) dz \\ = \int P(x|z)P(z) dz \\ = E_{z \in P(z)} [P(x|z)]
$$



### Monte-Carlro

확률 분포로부터 샘플링을 통해 f의 가중 평균을 구하는 것

- 샘플링을 통해서 이산분포인 것처럼 계산하여 근사 값을 찾아내는 것



## 03. Maximum Likelihood Estimation(MLE)

### 최대 우도법:

**모수적인 데이터 밀도 추정 방법**으로써 파라미터 θ=(θ1,⋯,θm)으로 구성된 어떤 **확률밀도함수 P(x|θ)**에서 관측된 표본 데이터 집합을 x=(x1,x2,⋯,xn)이라 할 때, 이 표본에서 파라미터 θ=(θ1,⋯,θm)를 추정하는 방법이다.



### 가능도(=우도, likelihood):

파라미터가 주어질 때 해당 표본이 수집될 확률

- 모델의 추정치가 잘 맞을 수록 '가능도가 높다'
- 지금 얻은 데이터가 이 분포로부터 나왔을 가능도



### Likelihood Function

- 각 데이터 샘플에서 후보 분포에 대한 높이(기여도)를 계산해서 다 곱한 것
- 입력으로 주어진 확률 분포(파라미터)가 데이터를 얼마나 잘 설명하는지 나타내는 점수(Likelihood)를 출력하는 함수
  - 입력: 확률 분포를 표현하는 파라미터
  - 출력: 데이터를 설명하는 정도(Prob Density)



전체 표본집합의 결합확률밀도 함수(Likelihood Function)
$$
P(x| \theta) = \Pi_{k=1}^n P(x_k| \theta)
$$


Log-likelihood Function
$$
L(\theta |x) = logP(x| \theta) = \sum_{i=0}^n logP(x_i| \theta)
$$

- 확률의 곱셉으로 이루어진 likelihood function은 수가 굉장히 작아지기 때문에 underflow의 가능성이 커진다.
- 또한, 로그를 사용하면 곱셈을 덧셈으로 연산해 속도가 빨라진다.

따라서, likelihood function의 최대 값을 가지는 파라미터의 확률 분포가 주어진 데이터를 가장 잘 설명한다고 볼 수 있다.

⇒ **Log-likelihood Function의 최대 값을 찾아야 한다.**

가장 보편적인 방법은 **미분계수**를 이용하는 것이다.

즉, 찾고자하는 파라미터에 **편미분**하고 그 값이 **0**이 되도록 하는 과정을 통해 파라미터를 찾을 수 있다.

랜덤하게 파라미터를 선택하는 것이 아닌 **Gradient Ascent**를 사용해 효율적으로 적절한 파라미터를 찾을 수 있다.



## 04. 신경망과 MLE

$$
P_\theta (x) = P(x; \theta) = P(x| \theta) \\ P_\theta (y|x) = P(y|x; \theta) = P(y|x, \theta)
$$



**Negative Log Likelihood(NLL)**

- 하지만 대부분의 딥러닝 프레임워크들은 Gradient Descent만 지원
- 따라서 maximization 문제에서 minimization 문제로 접근
- Gradient Descent를 수행하기 위해서 파라미터에 대한 미분이 필요함. 이를 효율적으로 수행하기 위해 back-propagation 활용

$$
\hat \theta  = argmax \sum_{i=0}^n logP(y_i|x_i ; \theta) \\ = argmin \sum_{i=1}^n -log P(y_i|x_i ; \theta)
$$



## 05. 수식 MLE

확률 값 구할 때 → **softmax**를 가지고 probability distribution을 계산한다.
$$
-\sum_{i=1}^N logP(y_i|x_i; \theta) = - \sum_{i=1}^N y_i^T \times log \hat y_i
$$
⇒ Cross Entropy Loss와 동일한 수식임을 알 수 있다.

⇒ 그동안 확률 값을 구하고 loss 값을 계산해왔던 과정이 MLE 였던 것을 알 수 있다.



## 06. Maximum A Posterior (MAP)

<img src="../Images/Bayes Theorem.png" style="zoom:60%;" />

### MAP Estimation

$$
\hat h = argmax P(h|D) \\ =argmax P(D|h)P(h)/P(D) \\ = argmaxP(D|h)P(h)
$$

*h에 관한 것이기 때문에 P(D)가 없는 것과 동일*



**Frequentist 관점**

- 파라미터는 최적화의 대상
- 현재까지의 정보를 바탕으로 추정
- Overfitting에 취약함

$$
\hat \theta = argmax P(D| \theta)
$$



**Bayesian 관점**

- 파라미터 또한 random variable이며, prior 분포를 따른다.
- 미래의 uncertainty까지 고려
- Prior에 대한 가정 필요

$$
\hat \theta = argmax P(\theta |D) \\ = argmax P(D| \theta)P(\theta)/P(D) \\ = argmaxP(D| \theta)P(\theta)
$$



하지만 둘 중에 어떤 관점이 낫다고 말할 수 없음.



## 07. KL-Divergence

- 두 분포 사이의 dissimilarity를 표현
- 거리라고 볼 수 없음 (KLD는 asymmetric하기 때문)

<img src="../Images/KL-Divergence.png" style="zoom:60%;" />

<img src="../Images/DNN Optimization_KLD.png" style="zoom:60%;" />

<img src="../Images/DNN Optimization_KLD(2).png" style="zoom:60%;" />

Gradient Descent로 KL-Divergence의 최소 값을 찾을 수 있다.



## 08. Information & Entropy

### Information

- Representation learning에 관해 다루다보니 자연스럽게 연결됨
- 불확실성(Uncertainty)을 나타내는 값

$$
I(X) = -logP(X)
$$



### Entropy

- 정보량의 기대값(평균)
- 분포의 평균적인 uncertainty를 나타내는 값
- 분포의 형태를 예측해볼 수 있다.

$$
H(P) = -E_{x \in P(X)} [logP(x)]
$$



### Cross Entropy

- 분포 P의 관점에서 본 분포 Q의 정보량의 평균
- 두 분포가 비슷할수록 작은 값을 가진다.

$$
H(P,Q) = -E_{x \in P(x)} [logG(x)]
$$



### 분류에서 loss 함수로 Cross Entropy를 쓰는 이유

$$
D_KL(P||Q) = -\sum _{c=1}^c P(x)[log(Q(x)) - log(P(x))] \\ = H(P,Q) - H(P)
$$

* KL-Divergence가 아닌 cross entropy를 쓰는 이유는 모분포(P)를 근사하기 위한 예측 분포(Q)이기 때문에 H(P)는 사실상 Q와 관련이 없다.

* Q의 parameter를 미분하면 H(P)는 사라지게 되고 그래서 H(P,Q)를 loss함수로 사용하게 되고, 이는 KL-Divergence를 사용한 것과 동일하다.



## 09. Appendix - MSE Loss

Regression 문제에서 쓰이는 MSE Loss 또한 확률적인 관점에서 똑같이 해석 가능

- 가우시안 분포를 따른다고 생각하면 MSE Loss
- Multi Nominal는 Cross Entropy



## 10. 정리하며

사실은 확률 분포 함수를 모사하는 것이 목표다!

확률 분포 P(x)에서 수집한 입력 데이터 x에 대해서 원하는 조건부 확률 분포 P(y|x) 또는 샘플링한 출력 데이터 y를 반환하도록, 손실함수를 최소화하는 확률 분포 함수의 파라미터를 찾자.

Gradient descent를 수행하기 위해 back-propagation을 수행하자.