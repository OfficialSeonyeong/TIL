# 03. Probabilistic Perspective



## 01. 들어가며

- 이 세상은 확률에 기반한다. → **확률 분포**를 학습해야 한다.
- 가상의 확률 분포 함수 `P(y|x)`를 근사할 것.



## 02. 기본 확률 통계

- 어떤 변수 x가 `x`라는 값을 가질 확률  `P(x = x)`
- 확률 분포 (함수)
  - 입력: 확률 변수 x
  - 출력: x가 각 값에 해당될 때에 대한 확률 값



### Discrete Probability Distribution

- 확률 값의 총 합은 1
- Probability Mass Function (확률 질량 함수, PMF)



### Continuous Probability Distribution

- 면적의 합이 1
- 함수 값이 1보다 클 수 있다.
- Probability Density Function (확률 밀도 함수, PDF)
- 어떤 샘플이 주어졌을 때, 확률 값을 알 수 없다.



### Joint Probability

결합 분포 `P(x,y)`



### Conditional Probability

조건부 확률 분포 `P(y|x) = P(x,y)/P(x)`  ⇒ `P(x,y) = P(y|x) P(x)`



### Bayes Theorem

데이터 D가 주어졌을 때, 가설 h의 확률

```
P(h|D) = P(D|h)P(h) / P(D)
```



### Marginal Distribution

결합 분포에서 한 변수를 적분한 형태
$$
P(x) = \int P(x,z) dz \\ =\int P(x|z)P(z) dz \\ = \int P(z|x)P(x) dz = P(x) \int P(z|x) dz
$$


### Expectation and Sampling

$$
E_{x \in P(x)} [f(x)] = \sum_{x \in X} P(x) \times f(x)\\P(x) = \int P(x,z) dz \\ = \int P(x|z)P(z) dz \\ = E_{z \in P(z)} [P(x|z)]
$$



### Monte-Carlro

확률 분포로부터 샘플링을 통해 f의 가중 평균을 구하는 것

- 샘플링을 통해서 이산분포인 것처럼 계산하여 근사 값을 찾아내는 것



## 03. Maximum Likelihood Estimation(MLE)

### 최대 우도법:

**모수적인 데이터 밀도 추정 방법**으로써 파라미터 θ=(θ1,⋯,θm)으로 구성된 어떤 **확률밀도함수 P(x|θ)**에서 관측된 표본 데이터 집합을 x=(x1,x2,⋯,xn)이라 할 때, 이 표본에서 파라미터 θ=(θ1,⋯,θm)를 추정하는 방법이다.



### 가능도(=우도, likelihood):

파라미터가 주어질 때 해당 표본이 수집될 확률

- 모델의 추정치가 잘 맞을 수록 '가능도가 높다'
- 지금 얻은 데이터가 이 분포로부터 나왔을 가능도



### Likelihood Function

- 각 데이터 샘플에서 후보 분포에 대한 높이(기여도)를 계산해서 다 곱한 것
- 입력으로 주어진 확률 분포(파라미터)가 데이터를 얼마나 잘 설명하는지 나타내는 점수(Likelihood)를 출력하는 함수
  - 입력: 확률 분포를 표현하는 파라미터
  - 출력: 데이터를 설명하는 정도(Prob Density)