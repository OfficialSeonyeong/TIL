{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 선형 회귀(Linear Regression)\n",
    "* 변수 x의 값은 독립적으로 변할 수 있는 것에 반해, y값은 계속해서 x의 값에 의해서, 종속적으로 결정되므로 x를 독립 변수, y를 종속 변수라고 한다.\n",
    "* 선형 회귀는 한 개 이상의 독립 변수 x와 y의 선형 관계를 모델링한다.\n",
    "* 만약, 독립 변수 x가 1개라면 단순 선형 회귀라고 한다.\n",
    "\n",
    "\n",
    "### 1.  단순 선형 회귀 분석(Simple Linear Regression Analysis)\n",
    "\n",
    "    y=Wx+b\n",
    "* 독립 변수 x와 곱해지는 값 W를 머신 러닝에서는 가중치(weight)\n",
    "* 별도로 더해지는 값 b는 편향(bias)\n",
    "\n",
    "\n",
    "### 2. 다중 선형 회귀 분석(Multiple Linear Regression Analysis)\n",
    "\n",
    "    y=W1x1+W2x2+...Wnxn+b\n",
    "* y는 2개, x는 1개가 아닌 여러 개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 가설(Hypothesis) 세우기\n",
    "x 와 y의 관계를 유추하기 위해서 수학적으로 식을 세워보게 되는데 머신 러닝에서는 이러한 식을 가설(Hypothesis)라고 한다.\n",
    "\n",
    "H(x)=Wx+b\n",
    "\n",
    "\n",
    "* 선형 회귀는 주어진 데이터로부터 y와 x의 관계를 가장 잘 나타내는 직선을 그리는 일\n",
    "* 어떤 직선인지 결정하는 것은 W와 b의 값이므로 선형 회귀에서 해야할 일은 결국 적절한 W와 b를 찾아내는 일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 비용 함수(Cost function) : 평균 제곱 오차(MSE)\n",
    "* 머신 러닝은 W와 b를 찾기 위해서 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세우고, 이 식의 값을 최소화하는 최적의 W와 b를 찾아낸다.\n",
    "* 실제값과 예측값에 대한 오차에 대한 식\n",
    "    * 목적 함수(Objective function): 함수의 값을 최소화하거나, 최대화하거나 하는 목적을 가진 함수\n",
    "    * 비용 함수(Cost function) 또는 손실 함수(Loss function): 값을 최소화하려는 함수\n",
    "    * 회귀 문제의 경우에는 주로 **평균 제곱 오차(Mean Squared Error, MSE)**가 사용\n",
    "* 오차(error): 주어진 데이터에서 각 x에서의 실제값 y와 위의 직선에서 예측하고 있는 H(x)값의 차이\n",
    "* 평균 제곱 오차: 모든 오차를 제곱하여 더한 뒤 데이터의 개수로 나눈, 오차의 제곱합에 대한 평균\n",
    "* 평균 제곱 오차의 값을 최소값으로 만드는 W와 b를 찾아내야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 옵티마이저(Optimizer) : 경사하강법(Gradient Descent)\n",
    "* 선형 회귀를 포함한 수많은 머신 러닝, 딥 러닝의 학습은 결국 비용 함수를 최소화하는 매개 변수인 W와 b을 찾기 위한 작업을 수행한다. 이때 사용되는 알고리즘을 **옵티마이저(Optimizer)** 또는 **최적화 알고리즘**이라고 한다.\n",
    "* 이 옵티마이저를 통해 적절한 W와 b를 찾아내는 과정을 머신 러닝에서 학습(training)이다.\n",
    "* 가장 기본적인 옵티마이저 알고리즘이 경사 하강법이다.\n",
    "* W는 머신 러닝 용어로 가중치이면서, 직선의 방정식 관점에서는 직선의 기울기이다.\n",
    "* 기울기가 지나치게 크면 실제값과 예측값의 오차가 커지고, 기울기가 지나치게 작아도 실제값과 예측값의 오차가 커진다.\n",
    "* b 또한 지나치게 크거나 작으면 오차가 커진다.\n",
    "* W와 cost(비용 함수의 값)의 관계는 2차 함수 모양이 되고, cost가 최소화가 되는 지점은 접선의 기울기가 0이 되는 지점이며, 또한 미분값이 0이 되는 지점이 된다.\n",
    "* 학습률(learning rate) α 는 W 를 그래프의 한 점으로보고 접선의 기울기가 0일 때까지 경사를 따라 내려간다는 관점에서는 얼마나 큰 폭으로 이동할지를 결정"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
