{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 순전파(Foward Propagation)\n",
    "활성화 함수, 은닉층의 수, 각 은닉층의 뉴런 수 등 딥 러닝 모델을 설계하고 나면 입력값은 입력층, 은닉층을 지나면서 각 층에서의 가중치와 함께 연산되며 출력층으로 향한다. 그리고 출력층에서 모든 연산을 마친 예측값이 나온다. 이와 같이 입력층에서 출력층 방향으로 예측값의 연산이 진행되는 과정을 **순전파**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 손실 함수(Loss function)\n",
    "손실 함수: 실제값과 예측값의 차이(오차)를 수치화하는 함수\n",
    "* 오차가 클수록 손실 함수의 값은 커지고, 오차가 작을수록 손실 함수의 값은 작아진다.\n",
    "* 회귀에서는 평균 제곱 오차(MSE), 분류 문제에서는 크로스 엔트로피(Cross-Entropy)를 주로 사용\n",
    "\n",
    "\n",
    "keras의 model.compile()에서 \n",
    "* 이진 분류일 경우 => binary_crossentropy\n",
    "\n",
    "```model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])```\n",
    "* 다중 클래스 분류일 경우 => categorical_crossentropy\n",
    "\n",
    "```model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 옵티마이저(Optimizer)\n",
    "* 손실 함수의 값을 줄여나가면서 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라진다.\n",
    "* **배치(Batch)**는 가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양\n",
    "\n",
    "### 1) 배치 경사 하강법(Batch Gradient Descent)\n",
    "* 오차(loss)를 구할 때 전체 데이터를 고려\n",
    "* 한 번의 에포크에 모든 매개변수 업데이트를 단 한 번 수행\n",
    "* 전체 데이터를 고려해서 학습하므로 에포크당 시간이 오래 걸리고 메모리를 크게 요구하지만, 글로벌 미니멈을 찾을 수 있다.\n",
    "\n",
    "```model.fit(X_train, y_train, batch_size=len(train_X))```\n",
    "\n",
    "### 2) 확률적 경사 하강법(Stochastic Gradient Descent, SGD)\n",
    "* 매개변수 값을 조정 시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법\n",
    "* 기존의 배치 경사 하강법보다 정확도가 낮을 수 있지만 속도가 빠르다.\n",
    "\n",
    "```model.fit(X_train, y_train, batch_size=1)```\n",
    "\n",
    "### 3) 미니 배치 경사 하강법(Mini-Batch Gradient Descent)\n",
    "* 정해진 양에 대해서만 계산하여 매개 변수의 값을 조정하는 경사 하강법\n",
    "* 전체 데이터를 계산하는 것보다 빠르고 SGD보다는 안정적\n",
    "* 실제 가장 많이 사용되는 경사 하강법\n",
    "\n",
    "```model.fit(X_train, y_train, batch_size=32) #32를 배치 크기로 하였을 경우```\n",
    "\n",
    "### 4) 모멘텀(Momentum)\n",
    "* 관성이라는 물리학의 법칙을 응용한 방법\n",
    "* 경사 하강법에서 계산된 접선의 기울기에 한 시점(step) 전의 접선의 기울기값을 일정한 비율만큼 반영 => 로컬 미니멈에 도달했을 때 관성의 힘을 빌리면 로컬 미니멈에서 탈출하는 효과를 얻는다.\n",
    "\n",
    "```keras.optimizers.SGD(lr = 0.01, momentum= 0.9)```\n",
    "\n",
    "### 5) 아다그라드(Adagrad)\n",
    "* 아다그라드는 각 매개변수에 서로 다른 학습률을 적용\n",
    "* 변화가 많은 매개변수는 학습률이 작게 설정되고 변화가 적은 매개변수는 학습률을 높게 설정\n",
    "\n",
    "```keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)```\n",
    "\n",
    "### 6) 알엠에스프롭(RMSprop)\n",
    "* 아다그라드는 학습을 계속 진행한 경우에는 나중에 학습률이 지나치게 떨어진다는 단점이 있는데 이를 다른 수식으로 대체했다.\n",
    "\n",
    "```keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)```\n",
    "\n",
    "### 7) 아담(Adam)\n",
    "* 알엠에스프롭과 모멘텀 두 가지를 합친 듯한 방법, 방향과 학습률 두 가지를 모두 잡기 위한 방법\n",
    "\n",
    "```keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 에포크와 배치 크기와 이터레이션(Epochs and Batch size and Iteration)\n",
    "\n",
    "### 1) 에포크(Epoch)\n",
    "* 에포크란 인공 신경망에서 전체 데이터에 대해서 순전파와 역전파가 끝난 상태이다.\n",
    "* 에포크 횟수가 지나치거나 너무 적으면 앞서 배운 과적합과 과소적합이 발생\n",
    "\n",
    "### 2) 배치 크기(Batch size)\n",
    "* 배치 크기란 몇개의 데이터 단위로 매개변수를 업데이트 하는지를 말한다.\n",
    "\n",
    "### 3) 이터레이션(Iteration)\n",
    "* 이터레이션이란 한 번의 에포크를 끝내기 위해서 필요한 배치의 수\n",
    "* 전체 데이터에서 배치 크기를 나눠준 값\n",
    "* 한 번의 에포크 당 매개변수 업데이트가 이뤄지는 횟수"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
