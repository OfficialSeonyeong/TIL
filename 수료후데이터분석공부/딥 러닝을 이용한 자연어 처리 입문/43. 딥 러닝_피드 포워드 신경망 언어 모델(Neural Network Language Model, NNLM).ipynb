{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 기존 N-gram 언어 모델의 한계\n",
    "* 문장에 확률을 할당하는 모델\n",
    "* 주어진 문맥으로부터 아직 모르는 단어를 예측하는 것을 언어 모델링이라고 한다.\n",
    "* n-gram 언어 모델은 언어 모델링에 바로 앞의 n-1개의 단어만 참고\n",
    "* 하지만 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 **희소 문제(sparsity problem)** 발생\n",
    "\n",
    "\n",
    "## 2. 단어의 의미적 유사성\n",
    "* 희소 문제는 기계가 단어 간 유사도를 알 수 있다면 해결할 수 있다.\n",
    "* => 단어 간 유사도를 반영한 벡터를 만드는 **워드 임베딩(word embedding)**\n",
    "\n",
    "\n",
    "## 3. 피드 포워드 신경망 언어 모델(NNLM)\n",
    "* 훈련 코퍼스 준비 -> 모든 단어를 숫자로 인코딩/원핫 인코딩 => NNLM의 입력이면서 예측을 위한 레이블\n",
    "* NNLM은 정해진 n개의 단어만 참고\n",
    "* 원핫 벡터를 입력 받은 NNLM은 다음층인 투사층(projection layer)을 지난다.\n",
    "* 보통 은닉층이라고 부르지만 여기서는 투사층으로 일반 은닉층과 다르게 가중치 행렬과의 연산은 이루어지지만 활성화 함수가 존재하지 않는다.\n",
    "* 투사층의 크기를 M으로 설정하면, 각 입력 단어들은 투사층에서 V X M 크기의 가중치 행렬과 곱해진다.\n",
    "* 원핫 벡터와 가중치 W 행렬의 곱은 W행렬의 i번째 행을 그대로 읽어오는 것과 동일해 lookup table이라고 부른다.\n",
    "* 룩업 테이블 작업을 거치면 V의 차원을 가지는 원핫 벡터는 이보다 더 차원이 작은 M차원의 단어 벡터로 맵핑된다.이 벡터들은 초기에는 랜덤한 값을 가지지만 학습 과정에서 값이 계속 변경되는데 이 단어 벡터를 **임베딩 벡터**라고 한다.\n",
    "* 각 단어가 테이블 룩업을 통해 임베딩 벡터로 변경되고 투사층에서 모든 임베딩 벡터들의 값은 연결(concatenation)이 된다.\n",
    "* 투사층은 활성화 함수가 존재하지 않는 선형층(linear layer)이고 그 다음부터는 다시 은닉층 사용\n",
    "* 투사층의 결과는 h의 크기를 가지는 은닉층을 지난다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
