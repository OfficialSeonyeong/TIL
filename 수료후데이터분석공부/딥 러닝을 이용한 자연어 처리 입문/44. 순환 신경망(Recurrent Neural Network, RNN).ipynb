{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RNN(Recurrent Neural Network)은 시퀀스(Sequence) 모델\n",
    "* 입력과 출력을 시퀀스 단위로 처리하는 모델\n",
    "* 은닉층에서 활성화 함수를 지난 값은 출력층 방향으로 향하는 **피트 포워드 신경망(Feed Forward Neural Network)**\n",
    "* **RNN**은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보낸다.\n",
    "* RNN에서 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 셀(cell)이라고 한다. => 이 셀은 이전의 값을 기억하려고 하는 일종의 메모리 역할을 수행하므로 이를 **메모리 셀 또는 RNN 셀**이라고 표현\n",
    "* 은닉층의 메모리 셀은 각각의 시점에서 바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용하는 재귀적 활동을 하고 있다.\n",
    "* 메모리 셀이 출력층 방향으로 또는 다음 시점의 자신에게 보내는 값을 **은닉 상태(hidden state)**라고 한다.\n",
    "* RNN은 입력과 출력의 길이를 다르게 설계 할 수 있으므로 다양한 용도로 사용할 수 있다.\n",
    "    * 하나의 입력에 대해서 여러개의 출력(one-to-many)의 모델은 하나의 이미지 입력에 대해서 사진의 제목을 출력하는 이미지 캡셔닝(Image Captioning) 작업에 사용\n",
    "    * 단어 시퀀스에 대해서 하나의 출력(many-to-one)을 하는 모델은 판별하는 감성 분류(sentiment classification), 또는 스팸 메일 분류(spam detection)에 사용\n",
    "    * 다 대 다(many-to-many)의 모델의 경우에는 입력 문장으로 부터 대답 문장을 출력하는 챗봇과 입력 문장으로부터 번역된 문장을 출력하는 번역기, 또는 개체명 인식이나 품사 태깅과 같은 작업에 사용\n",
    "    \n",
    "## 1. 케라스(Keras)로 RNN 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 층을 추가하는 코드\n",
    "model.add(SimpleRNN(hidden_size)) # 가장 간단한 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가 인자를 사용할 때\n",
    "model.add(SimpleRNN(hidden_size, input_shape=(timesteps, input_dim)))\n",
    "\n",
    "# 다른 표기\n",
    "model.add(SimpleRNN(hidden_size, input_length=M, input_dim=N))\n",
    "# 단, M과 N은 정수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* hidden_size = 은닉 상태의 크기를 정의. 메모리 셀이 다음 시점의 메모리 셀과 출력층으로 보내는 값의 크기(output_dim)와도 동일. RNN의 용량(capacity)을 늘린다고 보면 되며, 중소형 모델의 경우 보통 128, 256, 512, 1024 등의 값을 가진다.\n",
    "* timesteps = 입력 시퀀스의 길이(input_length)라고 표현하기도 함. 시점의 수.\n",
    "* input_dim = 입력의 크기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. RNN 층은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받는다.\n",
    "2. RNN층은 사용자의 설정에 따라 두 가지 종류의 출력을 내보낸다. \n",
    "    * 메모리 셀의 **최종 시점의 은닉 상태**만을 리턴하고자 한다면 (batch_size, output_dim) 크기의 2D 텐서를 리턴\n",
    "    * 메모리 셀의 **각 시점(time step)의 은닉 상태값들을 모아서** 전체 시퀀스를 리턴하고자 한다면 (batch_size, timesteps, output_dim) 크기의 3D 텐서를 리턴\n",
    "    * 이는 return_sequences = True를 설정하여 설정 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 파이썬으로 RNN 구현하기\n",
    "직접 Numpy로 RNN 층 구현\n",
    "\n",
    "은닉 상태를 계산하는 식은 다음과 같이 정의\n",
    "\n",
    "ht = tanh(WxXt + WhHt-1 + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state_t = 0 # 초기 은닉 상태를 0(벡터)로 초기화\n",
    "for input_t in input_length: # 각 시점마다 입력을 받는다.\n",
    "    output_t = tanh(input_t, hidden_state_t)  # 각 시점에 대해서 입력과 이전 은닉 상태를 가지고 연산\n",
    "    hidden_state_t = output_t  # 계산 결과는 현재 시점의 은닉 상태가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래의 코드는 이해를 돕기 위해 (timesteps, input_dim) 크기의 2D 텐서를 입력으로 받았다고 가정\n",
    "# 주의: 실제로 케라스에서는 (batch_size, timesteps, input_dim)의 크기의 3D 텐서를 입력으로 받는다.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "timesteps = 10 # 시점의 수. NLP에서는 보통 문장의 길이\n",
    "input_dim = 4 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원\n",
    "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량\n",
    "\n",
    "inputs = np.random.random((timesteps, input_dim)) # 입력에 해당되는 2D 센서\n",
    "\n",
    "hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태는 0(벡터)로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 초기 은닉 상태 출력\n",
    "print(hidden_state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치와 편향 정의\n",
    "Wx = np.random.random((hidden_size, input_dim))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치\n",
    "Wh = np.random.random((hidden_size, hidden_size)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.\n",
    "b = np.random.random((hidden_size,)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 8)\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(Wx))\n",
    "print(np.shape(Wh))\n",
    "print(np.shape(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(2, 8)\n",
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n",
      "(8, 8)\n",
      "(9, 8)\n",
      "(10, 8)\n",
      "[[0.90698374 0.77588342 0.8942538  0.94441418 0.97800182 0.88404799\n",
      "  0.70797986 0.75290383]\n",
      " [0.99976672 0.99896207 0.99997459 0.99979447 0.99997169 0.99994509\n",
      "  0.99981332 0.99988506]\n",
      " [0.99989697 0.99938303 0.99999383 0.99995181 0.9999719  0.99997915\n",
      "  0.99995503 0.99990934]\n",
      " [0.99995314 0.99951058 0.99999535 0.99995703 0.99998766 0.99998442\n",
      "  0.99997367 0.99993953]\n",
      " [0.99995423 0.99940062 0.99999389 0.99994735 0.99998385 0.99998169\n",
      "  0.99996881 0.99991108]\n",
      " [0.99996488 0.99943566 0.99999433 0.99992354 0.99999089 0.99998549\n",
      "  0.99997617 0.99996546]\n",
      " [0.99991352 0.99945981 0.99999255 0.9999052  0.99998572 0.99998211\n",
      "  0.99995974 0.99996351]\n",
      " [0.99992463 0.99958531 0.99999567 0.99994143 0.9999904  0.99998659\n",
      "  0.99997252 0.99997829]\n",
      " [0.99991222 0.99954248 0.99999544 0.99996423 0.99998179 0.99998268\n",
      "  0.99996506 0.9999294 ]\n",
      " [0.9998747  0.99976775 0.99999321 0.999954   0.99998998 0.99998052\n",
      "  0.99995117 0.99990886]]\n"
     ]
    }
   ],
   "source": [
    "# RNN 층\n",
    "\n",
    "total_hidden_states = []\n",
    "\n",
    "# 메모리 셀 동작\n",
    "for input_t in inputs:  # 각 시점에 따라서 입력값이 입력\n",
    "    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t)+b)  # Wx * Xt + Wh * Ht-1 + b(bias) \n",
    "    total_hidden_states.append(list(output_t)) # 각 시점의 은닉 상태 값을 계속 축적\n",
    "    print(np.shape(total_hidden_states)) # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)\n",
    "    hidden_state_t = output_t\n",
    "    \n",
    "total_hidden_states = np.stack(total_hidden_states, axis=0)\n",
    "# 출력 시 값을 깔끔하게 해준다.\n",
    "\n",
    "print(total_hidden_states)  # (timesteps, output_dim)의 크기. 이 경우 (10, 8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 깊은 순환 신경망(Deep Recurrent Neural Network)\n",
    "RNN도 다수의 은닉층을 가질 수 있다.\n",
    "```model = Sequential()```\n",
    "\n",
    "```model.add(SimpleRNN(hidden_size, return_sequences=True))```\n",
    "\n",
    "```model.add(SimpleRNN(hidden_size, return_sequences=True))```\n",
    "\n",
    "\n",
    "## 4. 양방향 순환 신경망(Bidirectional Recurrent Neural Network)\n",
    "* 양방향 순환 신경망은 시점 t에서의 출력값을 예측할 때 이전 시점의 데이터뿐만 아니라, 이후 데이터로도 예측할 수 있다는 아이디어에 기반\n",
    "* 양방향 RNN은 하나의 출력값을 예측하기 위해 기본적으로 두 개의 메모리 셀을 사용\n",
    "* 첫번째 메모리 셀은 **앞 시점의 은닉 상태(Forward States)**를 전달받아 현재의 은닉 상태를 계산\n",
    "* 두번째 메모리 셀은 **뒤 시점의 은닉 상태(Backward States)**를 전달 받아 현재의 은닉 상태를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences=True), input_shape=(timesteps, input_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양방향 RNN도 다수의 은닉층을 가질 수 있다.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True), input_shape=(timesteps, input_dim)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
