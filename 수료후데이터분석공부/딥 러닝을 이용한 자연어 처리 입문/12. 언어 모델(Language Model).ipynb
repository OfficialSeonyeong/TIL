{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 언어 모델(Language Model)\n",
    "\n",
    "* 단어 시퀀스(문장)에 확률을 할당하는 모델\n",
    "* 통계를 이용한 방법 / 인공 신경망을 이용한 방법\n",
    "\n",
    "\n",
    "* 통계에 기반한 전통적인 언어 모델(Statistical Languagel Model, SLM)은 우리가 실제 사용하는 자연어를 근사하기에는 많은 한계가 있다.\n",
    "* 요즘 들어 인공 신경망이 그러한 한계를 해결해 통계 기반 언어 모델의 사용 용도가 줄었다.\n",
    "* 하지만, n-gram 은 활발하게 활용되고 있으며, 통계 기반 방법론에 대한 이해는 언어 모델에 대한 전체적인 시야를 갖게 해준다.\n",
    "\n",
    "**언어 모델링(Language Modeling)**\n",
    "\n",
    "주어진 단어들로부터 아직 모르는 단어를 예측하는 작업\n",
    "\n",
    "즉, 언어 모델이 이전 단어들로부터 다음 단어를 예측하는 일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 단어 시퀀스의 확률 할당\n",
    "a. 기계 번역(Machine Translation)\n",
    "\n",
    "P(나는 버스를 탔다) > P(나는 버스를 태운다)\n",
    "\n",
    ": 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단한다.\n",
    "\n",
    "b. 오타 교정(Spell Correction)\n",
    "\n",
    "선생님이 교실로 부리나케\n",
    "P(달려갔다) > P(잘려갔다)\n",
    "\n",
    ": 언어 모델은 두 문장을 비교하여 좌측의 문장의 확률이 더 높다고 판단한다.\n",
    "\n",
    "c. 음성 인식(Speech Recognition)\n",
    "\n",
    "P(나는 메롱을 먹는다) < P(나는 메론을 먹는다)\n",
    "\n",
    ": 언어 모델은 두 문장을 비교하여 우측의 문장의 확률이 더 높다고 판단한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 주어진 이전 단어들로부터 다음 단어 예측하기\n",
    "=> 이를 조건부 확률로 표현\n",
    "\n",
    "A. 단어 시퀀스의 확률\n",
    "\n",
    "* 하나의 단어를 w, 단어 시퀀스을 대문자 W라고 한다면, n개의 단어가 등장하는 단어 시퀀스 W의 확률\n",
    "* P(W)=P(w1,w2,w3,w4,w5,...,wn)\n",
    "\n",
    "B. 다음 단어 등장 확률\n",
    "\n",
    "* n -1개의 단어가 나열된 상태에서 n번째 단어의 확률\n",
    "* P(wn|w1,...,wn−1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
