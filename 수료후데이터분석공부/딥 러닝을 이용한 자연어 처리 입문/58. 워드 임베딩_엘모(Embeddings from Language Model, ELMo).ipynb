{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"58. 워드 임베딩_엘모(Embeddings from Language Model, ELMo).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOjszbDysBhD2IKimEx4063"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"J4jescc37uos"},"source":["ELMo의 가장 큰 특징은 **사전 훈련된 언어 모델(Pre-trained language model)**을 사용한다는 점이다.\n","*하지만 현재 텐서플로우 2.0에서는 사용 불가, 텐서플로우 버전 1버전에서 사용해야*"]},{"cell_type":"markdown","metadata":{"id":"Vdk6C4K77_1q"},"source":["## 1. ELMo(Embeddings from Language Model)\n","같은 표기의 단어인데 문맥에 따라 전혀 다른 의미로 사용되는 것은 Word2Vec와 GloVe가 제대로 반영하지 못한다.\n","\n","**문맥을 반영한 워드 임베딩(Contextualized Word Embedding)** 등장\n"]},{"cell_type":"markdown","metadata":{"id":"1nMLZ6dL9kPM"},"source":["## 2. biLM(Bidirectional Language Model)의 사전 훈련\n","* biLM은 기본적으로 다층 구조(Multi-layer)를 전제로 한다.\n","* char CNN이라는 방법을 사용해 글자(character) 단위로 계산, 서브단어(subword)의 정보를 참고하는 것처럼 문맥과 상관없이 단어 간의 연관성을 찾아낸다.\n","* OOV에도 견고하다.\n","\n","*양방향 RNN가 다르다. 양방향 RNN은 순방향 RNN의 은닉 상태와 역방향의 RNN의 은닉 상태를 다음 층의 입력으로 보내기 전에 연결(concatenate). biLM의 순방향 언어모델과 역방향 언어모델이 각각의 은닉 상태만을 다음 은닉층으로 보내며 훈련시킨 후에 ELMo 표현으로 사용하기 위해서 은닉 상태를 연결(concatenate).*\n"]},{"cell_type":"code","metadata":{"id":"dRHM2AQs6Mk6"},"source":[""],"execution_count":null,"outputs":[]}]}